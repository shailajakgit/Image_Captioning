{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4781a0-901f-4c31-a5e8-1575cf6de343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:35:59.471388Z",
     "iopub.status.busy": "2022-07-06T01:35:59.470744Z",
     "iopub.status.idle": "2022-07-06T01:36:03.680184Z",
     "shell.execute_reply": "2022-07-06T01:36:03.679263Z",
     "shell.execute_reply.started": "2022-07-06T01:35:59.471306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.7.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 19.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.8/site-packages (from h5py) (1.22.2)\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-3.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56778d6d-2789-4f81-a7ea-fe9735ced410",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:03.682326Z",
     "iopub.status.busy": "2022-07-06T01:36:03.682068Z",
     "iopub.status.idle": "2022-07-06T01:36:06.937434Z",
     "shell.execute_reply": "2022-07-06T01:36:06.936617Z",
     "shell.execute_reply.started": "2022-07-06T01:36:03.682286Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from utils.cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from utils.cs231n.rnn_layers import *\n",
    "#from utils.cs231n.captioning_solver import CaptioningSolver\n",
    "#from utils.cs231n.classifiers.rnn import CaptioningRNN\n",
    "from utils.cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from utils.cs231n.image_utils import image_from_url\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "import torch\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c6a8e5-9c04-490e-8ec1-d72880767a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:06.938960Z",
     "iopub.status.busy": "2022-07-06T01:36:06.938751Z",
     "iopub.status.idle": "2022-07-06T01:36:08.445953Z",
     "shell.execute_reply": "2022-07-06T01:36:08.445049Z",
     "shell.execute_reply.started": "2022-07-06T01:36:06.938937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
      "train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
      "val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
      "val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
      "train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
      "val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
      "idx_to_word <class 'list'> 1004\n",
      "word_to_idx <class 'dict'> 1004\n",
      "train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
      "val_urls <class 'numpy.ndarray'> (40504,) <U63\n"
     ]
    }
   ],
   "source": [
    "# Load COCO data from disk; this returns a dictionary\n",
    "# We'll work with dimensionality-reduced features for this notebook, but feel\n",
    "# free to experiment with the original features by changing the flag below.\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20e87d92-5e66-4a0d-b3c9-42a334fdaae2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:08.447875Z",
     "iopub.status.busy": "2022-07-06T01:36:08.447696Z",
     "iopub.status.idle": "2022-07-06T01:36:08.472854Z",
     "shell.execute_reply": "2022-07-06T01:36:08.472010Z",
     "shell.execute_reply.started": "2022-07-06T01:36:08.447853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbatch_size = 3\\ncaptions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\\nfor i, (caption, url) in enumerate(zip(captions, urls)):\\n    plt.imshow(image_from_url(url))\\n    plt.axis('off')\\n    caption_str = decode_captions(caption, data['idx_to_word'])\\n    plt.title(caption_str)\\n    plt.show()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample a minibatch and show the images and captions\n",
    "\n",
    "'''\n",
    "batch_size = 3\n",
    "captions, features, urls = sample_coco_minibatch(data, batch_size=batch_size)\n",
    "for i, (caption, url) in enumerate(zip(captions, urls)):\n",
    "    plt.imshow(image_from_url(url))\n",
    "    plt.axis('off')\n",
    "    caption_str = decode_captions(caption, data['idx_to_word'])\n",
    "    plt.title(caption_str)\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677077bc-452f-4255-b26a-3426f8453177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e839d23-5cf2-4830-b647-8bd7fd351e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:08.473812Z",
     "iopub.status.busy": "2022-07-06T01:36:08.473641Z",
     "iopub.status.idle": "2022-07-06T01:36:08.496394Z",
     "shell.execute_reply": "2022-07-06T01:36:08.495501Z",
     "shell.execute_reply.started": "2022-07-06T01:36:08.473790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncaption = ''\\nfor i in data['train_captions'][2]:\\n    caption = caption + dict_idx_to_word[i] +' '\\n    \\ncaption\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data['train_features'].shape\n",
    "# data['train_captions'].shape\n",
    "# len(data['word_to_idx'])\n",
    "# data['idx_to_word']\n",
    "# data['train_captions'][0]\n",
    "# dict_idx_to_word = {k:v for k,v in enumerate(data['idx_to_word'])}\n",
    "# dict_idx_to_word\n",
    "#for ind in data['train_captions'][0]\n",
    "# np.apply_along_axis(lambda x: dict_idx_to_word[x], axis=0, arr=data['train_captions'][0])\n",
    "'''\n",
    "caption = ''\n",
    "for i in data['train_captions'][2]:\n",
    "    caption = caption + dict_idx_to_word[i] +' '\n",
    "    \n",
    "caption\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0849983f-d705-4296-b606-f754e0938415",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:08.497626Z",
     "iopub.status.busy": "2022-07-06T01:36:08.497439Z",
     "iopub.status.idle": "2022-07-06T01:36:08.520258Z",
     "shell.execute_reply": "2022-07-06T01:36:08.519586Z",
     "shell.execute_reply.started": "2022-07-06T01:36:08.497604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nshow_img_ind = 234\\nprint(data['train_urls'][show_img_ind])\\nlst_captions = []\\nfor arr_ind,img_ind in enumerate(data['train_image_idxs']):\\n    if img_ind == show_img_ind:\\n        lst_captions.append(arr_ind)\\n\\nfor cap_ind in lst_captions:\\n    caption = ''\\n    for i in data['train_captions'][cap_ind]:\\n        caption = caption + dict_idx_to_word[i] +' '\\n    print(caption)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "show_img_ind = 234\n",
    "print(data['train_urls'][show_img_ind])\n",
    "lst_captions = []\n",
    "for arr_ind,img_ind in enumerate(data['train_image_idxs']):\n",
    "    if img_ind == show_img_ind:\n",
    "        lst_captions.append(arr_ind)\n",
    "\n",
    "for cap_ind in lst_captions:\n",
    "    caption = ''\n",
    "    for i in data['train_captions'][cap_ind]:\n",
    "        caption = caption + dict_idx_to_word[i] +' '\n",
    "    print(caption)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e331998a-7160-42f7-909e-4ce8c94818c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:08.521515Z",
     "iopub.status.busy": "2022-07-06T01:36:08.521328Z",
     "iopub.status.idle": "2022-07-06T01:36:08.543874Z",
     "shell.execute_reply": "2022-07-06T01:36:08.543299Z",
     "shell.execute_reply.started": "2022-07-06T01:36:08.521492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\n    def map_image_to_loc(self):\\n        if self.is_train:\\n            for loc_ind, img_ind in enumerate(self.dict_data['train_image_idxs']):\\n                try:\\n                    self.dict_image_to_loc[img_ind].append(loc_ind)\\n                except KeyError:\\n                    self.dict_image_to_loc[img_ind] = [loc_ind]\\n        else:\\n        `   for loc_ind, img_ind in enumerate(self.dict_data['val_image_idxs']):\\n                try:\\n                    self.dict_image_to_loc[img_ind].append(loc_ind)\\n                except KeyError:\\n                    self.dict_image_to_loc[img_ind] = [loc_ind]\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "        self.dict_image_to_loc = {}\n",
    "        if self.is_train:\n",
    "            self.map_image_to_loc()\n",
    "            self.dict_image_captions = dict(k:[] for k in self.arr_image_ind)\n",
    "'''\n",
    "'''    \n",
    "    def map_image_to_loc(self):\n",
    "        if self.is_train:\n",
    "            for loc_ind, img_ind in enumerate(self.dict_data['train_image_idxs']):\n",
    "                try:\n",
    "                    self.dict_image_to_loc[img_ind].append(loc_ind)\n",
    "                except KeyError:\n",
    "                    self.dict_image_to_loc[img_ind] = [loc_ind]\n",
    "        else:\n",
    "        `   for loc_ind, img_ind in enumerate(self.dict_data['val_image_idxs']):\n",
    "                try:\n",
    "                    self.dict_image_to_loc[img_ind].append(loc_ind)\n",
    "                except KeyError:\n",
    "                    self.dict_image_to_loc[img_ind] = [loc_ind]\n",
    "'''  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e16d3-133b-460c-b6ac-640f6a534b14",
   "metadata": {},
   "source": [
    "### RNN Strategy \n",
    "\n",
    "0. How to handle input to the model? Read image and all 5 captions or just the top 1? Output at each time step is the \n",
    "    softmax proba of the input word. \n",
    "    \n",
    "    Answer: Sample an image from 400,135 image-captions and present the associated caption, instead of fetching all 5 \n",
    "    captions of the sampled image.\n",
    "\n",
    "1. Word vectors: Random initialization?\n",
    "\n",
    "    Answer: Yes, through Embedding layer.\n",
    "\n",
    "2. Combining Image features with the word vectors at each time step.\n",
    "    \n",
    "    Image input becomes the hidden state to the first time step(h_t at t=1) and at every time step recieves an embedding (x_t)\n",
    "\n",
    "3. Creating y_t at each time step through softmax function\n",
    "\n",
    "4. Masking the input tokens at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f63344-adcc-4518-a59e-3470f2cf547d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:08.545267Z",
     "iopub.status.busy": "2022-07-06T01:36:08.545069Z",
     "iopub.status.idle": "2022-07-06T01:36:08.569594Z",
     "shell.execute_reply": "2022-07-06T01:36:08.568871Z",
     "shell.execute_reply.started": "2022-07-06T01:36:08.545234Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self,is_train=True):\n",
    "        \n",
    "        '''\n",
    "        train_captions <class 'numpy.ndarray'> (400135, 17) int32\n",
    "        train_image_idxs <class 'numpy.ndarray'> (400135,) int32\n",
    "        # val_captions <class 'numpy.ndarray'> (195954, 17) int32\n",
    "        # val_image_idxs <class 'numpy.ndarray'> (195954,) int32\n",
    "        train_features <class 'numpy.ndarray'> (82783, 512) float32\n",
    "        # val_features <class 'numpy.ndarray'> (40504, 512) float32\n",
    "        idx_to_word <class 'list'> 1004\n",
    "        word_to_idx <class 'dict'> 1004\n",
    "        train_urls <class 'numpy.ndarray'> (82783,) <U63\n",
    "        # val_urls <class 'numpy.ndarray'> (40504,) <U63\n",
    "        '''\n",
    "        \n",
    "        self.dict_data = load_coco_data(pca_features=True)\n",
    "        self.is_train = is_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return len(self.dict_data['train_image_idxs'])\n",
    "        else:\n",
    "            return len(self.dict_data['val_image_idxs'])\n",
    "        \n",
    "    def __getitem__(self,loc_idx):\n",
    "        \n",
    "        # Model input\n",
    "        if self.is_train:\n",
    "            image_ind = self.dict_data['train_image_idxs'][loc_idx]\n",
    "            self.arr_image_features = self.dict_data['train_features'][image_ind]\n",
    "            self.arr_caption_words = self.dict_data['train_captions'][loc_idx]\n",
    "        else:\n",
    "            image_ind = self.dict_data['val_image_idxs'][loc_idx]\n",
    "            self.arr_image_features = self.dict_data['val_features'][image_ind]\n",
    "            self.arr_caption_words = self.dict_data['val_captions'][loc_idx]\n",
    "        \n",
    "        # 0 = <NULL>\n",
    "        self.mask = (self.arr_caption_words!=0).astype(np.uint8)\n",
    "        \n",
    "        # model output - (L-1,V) for one sample\n",
    "        #self.arr_one_hot_caption_words = nn.functional.one_hot(torch.from_numpy(self.arr_caption_words[1:].astype(np.int64)),num_classes=1004 )\n",
    "        \n",
    "        # The model expects the output to be of shape (L,N,V)\n",
    "        #self.arr_one_hot_caption_words = torch.permute(self.arr_one_hot_caption_words,(1,0,2))\n",
    "        \n",
    "        # [(N,D), (N,L-1), (N, L-1), (N, L-1, V)]\n",
    "        #return (self.arr_image_features, self.arr_caption_words[:-1], self.mask[:-1], self.arr_one_hot_caption_words)\n",
    "        # [(N,D), (N,L-1), (N, L-1), (N, L-1)]\n",
    "        return (self.arr_image_features, self.arr_caption_words[:-1], self.mask[:-1], \n",
    "                torch.from_numpy(self.arr_caption_words[1:]).to(torch.long))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71d00dae-0061-4f9f-ae08-2e7453e710f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:08.570688Z",
     "iopub.status.busy": "2022-07-06T01:36:08.570463Z",
     "iopub.status.idle": "2022-07-06T01:36:08.594586Z",
     "shell.execute_reply": "2022-07-06T01:36:08.593851Z",
     "shell.execute_reply.started": "2022-07-06T01:36:08.570665Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningRNN(nn.Module):\n",
    "    def __init__(self, image_size, vocab_size, hidden_size, embedding_size):\n",
    "        super(ImageCaptioningRNN, self).__init__()\n",
    "        # V\n",
    "        self.vocab_size = vocab_size\n",
    "        #D\n",
    "        self.image_size = image_size\n",
    "        #H\n",
    "        self.hidden_size = hidden_size\n",
    "        #E\n",
    "        self.embedding_size = embedding_size\n",
    "        #L - Sequence length\n",
    "        # self.seq_len = seq_len\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(num_embeddings=self.vocab_size, \n",
    "                                            embedding_dim=self.embedding_size, \n",
    "                                            padding_idx=0)\n",
    "        # Affine transformation of image_vector is h_0\n",
    "        self.h_0 = nn.Sequential(\n",
    "            nn.Linear(self.image_size,self.hidden_size),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size =self.embedding_size,hidden_size =self.hidden_size, \n",
    "                          nonlinearity ='tanh',\n",
    "                         num_layers = 1)\n",
    "        \n",
    "        self.extend_rnn_output = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "        self.word_softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, image_vec, caption_int_vec):\n",
    "        # RNN requires a 3D h0 of shape (1,N,H) if its input is 3D.\n",
    "        h0 = self.h_0(image_vec)\n",
    "        h0 = torch.unsqueeze(h0,0)\n",
    "        # N, L, E\n",
    "        embeddings = self.word_embeddings(caption_int_vec)\n",
    "        \n",
    "        # L, N, E\n",
    "        embeddings = torch.permute(embeddings, (1,0,2))#torch.reshape(embeddings, (1,))\n",
    "        \n",
    "        ##L,N,H\n",
    "        rnn_output, h_n = self.rnn(embeddings, h0)\n",
    "        \n",
    "        # L, N, V\n",
    "        extended_rnn_output = self.extend_rnn_output(rnn_output)\n",
    "        word_logits = self.word_softmax(extended_rnn_output)\n",
    "        return word_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f703916a-90c0-4de5-a88c-52f6faead83f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:08.596976Z",
     "iopub.status.busy": "2022-07-06T01:36:08.596657Z",
     "iopub.status.idle": "2022-07-06T01:36:08.625135Z",
     "shell.execute_reply": "2022-07-06T01:36:08.624213Z",
     "shell.execute_reply.started": "2022-07-06T01:36:08.596952Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs, model, loss_fn, optim, train_dataloader, val_dataloader, vocab_size):\n",
    "    lst_train_loss = []\n",
    "    lst_valid_loss = []\n",
    "    # 1. model.train()\n",
    "    # model.train()\n",
    "    # 2. Iterate over epochs, batches\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_num, batch in enumerate(train_dataloader):\n",
    "            # 3. get logits\n",
    "            image = batch[0].cuda()\n",
    "            image.to(device)\n",
    "            \n",
    "            caption_words_int = batch[1].cuda()\n",
    "            caption_words_int.to(device)\n",
    "            \n",
    "            # (N,L,V) to (L,N,V). Permuting the output in train becuase\n",
    "            ## prior to creating batches, the data shape is (L,V) for one example\n",
    "            ## caption_words_onehot = batch[3]\n",
    "            ## caption_words_onehot = torch.permute(caption_words_onehot, (1,0,2))\n",
    "            ## caption_words_onehot = caption_words_onehot.cuda()\n",
    "            ## caption_words_onehot.to(device)\n",
    "            # Reshape the logits and target to (N*L,V) to feed the loss function.\n",
    "            ## logits = logits.reshape(-1, vocab_size)\n",
    "            ## caption_words_onehot = caption_words_onehot.reshape(-1,vocab_size)\n",
    "            # 4. Get CE error\n",
    "            ## loss = loss_fn(logits,caption_words_onehot)\n",
    "            \n",
    "            caption_words_target = batch[3].flatten().cuda()\n",
    "            caption_words_target.to(device)\n",
    "            \n",
    "            \n",
    "            # L, N, V\n",
    "            logits = model(image, caption_words_int)\n",
    "            # N,L,V through transpose then N*L, V through reshape\n",
    "            logits = logits.transpose(0,1).reshape(-1,vocab_size)\n",
    "            \n",
    "            # 4. Get CE error\n",
    "            loss = loss_fn(logits,caption_words_target)\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            # 5. backprop, step\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            # 6. print error at every 100th batch\n",
    "            if batch_num%100 == 0:\n",
    "                print(\"Training loss: \",loss.item())\n",
    "                lst_train_loss.append(loss.item())\n",
    "                \n",
    "        # 7. print val error at the end of each epoch\n",
    "        valid_loss = validate(model, loss_fn, optim, val_dataloader,vocab_size)\n",
    "        lst_valid_loss.append(valid_loss)\n",
    "    return lst_train_loss, lst_valid_loss, model\n",
    "        \n",
    "        \n",
    "def validate(model, loss_fn, optim, val_dataloader,vocab_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = len(val_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for image_vec, caption_word_int, mask, caption_target in val_dataloader:\n",
    "            \n",
    "            image_vec = image_vec.cuda()\n",
    "            image_vec.to(device)\n",
    "            \n",
    "            caption_word_int = caption_word_int.cuda() \n",
    "            caption_word_int.to(device)\n",
    "            \n",
    "            mask = mask.cuda() \n",
    "            mask.to(device)\n",
    "            \n",
    "            caption_target = caption_target.flatten().cuda()\n",
    "            caption_target.to(device)\n",
    "            '''\n",
    "            caption_onehot = caption_onehot.cuda()\n",
    "            logits = model(image_vec,caption_word_int)\n",
    "            logits = logits.reshape(-1, vocab_size)\n",
    "            caption_words_onehot = caption_words_onehot.reshape(-1,vocab_size)\n",
    "            loss = loss_fn(logits, caption_onehot)\n",
    "            '''\n",
    "            logits = model(image_vec,caption_word_int)\n",
    "            logits = logits.transpose(0,1).reshape(-1,vocab_size)\n",
    "            loss = loss_fn(logits, caption_target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    loss_val = total_loss/num_batches\n",
    "    print(\"Validation Batch Error: \", loss_val)\n",
    "    return loss_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ded172c-ad9f-4ff7-992f-e1ebe596d95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:08.626878Z",
     "iopub.status.busy": "2022-07-06T01:36:08.626695Z",
     "iopub.status.idle": "2022-07-06T01:36:08.648548Z",
     "shell.execute_reply": "2022-07-06T01:36:08.647914Z",
     "shell.execute_reply.started": "2022-07-06T01:36:08.626855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nword_embeddings = nn.Embedding(num_embeddings=1004, \\n                                            embedding_dim=300, \\n                                            padding_idx=0)\\n\\nembeddings = word_embeddings(caption_int_vec)\\n\\ndict_data = load_coco_data(pca_features=True)\\nimage_ind = dict_data['train_image_idxs'][6]\\narr_image_features = dict_data['train_features'][image_ind]\\narr_caption_words = dict_data['train_captions'][6]\\n\\n\\n# model output - (L-1,V)\\narr_one_hot_caption_words = nn.functional.one_hot(torch.from_numpy(arr_caption_words[1:].astype(np.int64)),num_classes=1004 )\\n\\n# The model expects the output to be of shape (L,N,V)\\n#arr_one_hot_caption_words = torch.permute(arr_one_hot_caption_words,(1,0,2))\\n\\n#arr_caption_words\\narr_one_hot_caption_words.shape\\n\\nbatch = next(iter(train_dataloader))\\nprint(batch[0].shape)\\nprint(batch[1].shape)\\nprint(batch[2].shape)\\nprint(batch[3].shape)\\ntarget = torch.empty(3, dtype=torch.long).random_(5)\\ntarget\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "word_embeddings = nn.Embedding(num_embeddings=1004, \n",
    "                                            embedding_dim=300, \n",
    "                                            padding_idx=0)\n",
    "\n",
    "embeddings = word_embeddings(caption_int_vec)\n",
    "\n",
    "dict_data = load_coco_data(pca_features=True)\n",
    "image_ind = dict_data['train_image_idxs'][6]\n",
    "arr_image_features = dict_data['train_features'][image_ind]\n",
    "arr_caption_words = dict_data['train_captions'][6]\n",
    "\n",
    "\n",
    "# model output - (L-1,V)\n",
    "arr_one_hot_caption_words = nn.functional.one_hot(torch.from_numpy(arr_caption_words[1:].astype(np.int64)),num_classes=1004 )\n",
    "\n",
    "# The model expects the output to be of shape (L,N,V)\n",
    "#arr_one_hot_caption_words = torch.permute(arr_one_hot_caption_words,(1,0,2))\n",
    "\n",
    "#arr_caption_words\n",
    "arr_one_hot_caption_words.shape\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(batch[0].shape)\n",
    "print(batch[1].shape)\n",
    "print(batch[2].shape)\n",
    "print(batch[3].shape)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "target\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dcb9675-4f87-49f6-90d3-c5145ca6a53a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:08.649628Z",
     "iopub.status.busy": "2022-07-06T01:36:08.649456Z",
     "iopub.status.idle": "2022-07-06T01:36:09.331025Z",
     "shell.execute_reply": "2022-07-06T01:36:09.330438Z",
     "shell.execute_reply.started": "2022-07-06T01:36:08.649605Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches:  3127\n",
      "Validation batches:  1531\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset = ImageCaptioningDataset(True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "print(\"Training batches: \",len(train_dataloader))\n",
    "\n",
    "val_dataset = ImageCaptioningDataset(False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = batch_size, shuffle=True)\n",
    "print(\"Validation batches: \", len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b002202-9f95-44f4-afa8-6188c514645e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:09.332233Z",
     "iopub.status.busy": "2022-07-06T01:36:09.332014Z",
     "iopub.status.idle": "2022-07-06T01:36:15.398919Z",
     "shell.execute_reply": "2022-07-06T01:36:15.398240Z",
     "shell.execute_reply.started": "2022-07-06T01:36:09.332210Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tunable\n",
    "hidden_size=256\n",
    "embedding_size=300\n",
    "\n",
    "# Fixed\n",
    "vocab_size=1004\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = ImageCaptioningRNN(image_size=512, \n",
    "                           vocab_size=vocab_size, \n",
    "                           hidden_size=hidden_size, \n",
    "                           embedding_size=embedding_size).to(device)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optim = Adam(model.parameters(),lr=0.001,weight_decay=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ded76ca-5d04-4da9-a53e-8a1d037896f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T01:36:15.400255Z",
     "iopub.status.busy": "2022-07-06T01:36:15.399983Z",
     "iopub.status.idle": "2022-07-06T02:01:02.638541Z",
     "shell.execute_reply": "2022-07-06T02:01:02.637644Z",
     "shell.execute_reply.started": "2022-07-06T01:36:15.400232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  6.911731243133545\n",
      "Training loss:  6.69919490814209\n",
      "Training loss:  6.811092853546143\n",
      "Training loss:  6.806044578552246\n",
      "Training loss:  6.805054664611816\n",
      "Training loss:  6.768823623657227\n",
      "Training loss:  6.7119832038879395\n",
      "Training loss:  6.454736232757568\n",
      "Training loss:  6.474460601806641\n",
      "Training loss:  6.461073875427246\n",
      "Training loss:  6.4475250244140625\n",
      "Training loss:  6.449706077575684\n",
      "Training loss:  6.4448161125183105\n",
      "Training loss:  6.447281837463379\n",
      "Training loss:  6.455914497375488\n",
      "Training loss:  6.450200080871582\n",
      "Training loss:  6.45078706741333\n",
      "Training loss:  6.45526123046875\n",
      "Training loss:  6.446138381958008\n",
      "Training loss:  6.45703125\n",
      "Training loss:  6.441700458526611\n",
      "Training loss:  6.448322296142578\n",
      "Training loss:  6.4478559494018555\n",
      "Training loss:  6.461270809173584\n",
      "Training loss:  6.46243953704834\n",
      "Training loss:  6.451048851013184\n",
      "Training loss:  6.443991184234619\n",
      "Training loss:  6.4551520347595215\n",
      "Training loss:  6.459887981414795\n",
      "Training loss:  6.4604597091674805\n",
      "Training loss:  6.459042549133301\n",
      "Training loss:  6.458375930786133\n",
      "Validation Batch Error:  6.454987577952878\n",
      "Training loss:  6.441495895385742\n",
      "Training loss:  6.44076681137085\n",
      "Training loss:  6.778249740600586\n",
      "Training loss:  6.812935829162598\n",
      "Training loss:  6.8140130043029785\n",
      "Training loss:  6.820383548736572\n",
      "Training loss:  6.813509464263916\n",
      "Training loss:  6.816917419433594\n",
      "Training loss:  6.819602012634277\n",
      "Training loss:  6.814544200897217\n",
      "Training loss:  6.814040184020996\n",
      "Training loss:  6.808689594268799\n",
      "Training loss:  6.807683944702148\n",
      "Training loss:  6.80879545211792\n",
      "Training loss:  6.815029144287109\n",
      "Training loss:  6.816552639007568\n",
      "Training loss:  6.810579776763916\n",
      "Training loss:  6.807663440704346\n",
      "Training loss:  6.805261611938477\n",
      "Training loss:  6.819000720977783\n",
      "Training loss:  6.811647415161133\n",
      "Training loss:  6.8087005615234375\n",
      "Training loss:  6.818331718444824\n",
      "Training loss:  6.817929744720459\n",
      "Training loss:  6.807223320007324\n",
      "Training loss:  6.814120769500732\n",
      "Training loss:  6.815045356750488\n",
      "Training loss:  6.812131881713867\n",
      "Training loss:  6.812562465667725\n",
      "Training loss:  6.8184733390808105\n",
      "Training loss:  6.812577247619629\n",
      "Training loss:  6.819854259490967\n",
      "Validation Batch Error:  6.812231000306013\n",
      "Training loss:  6.80679988861084\n",
      "Training loss:  6.813218116760254\n",
      "Training loss:  6.818489074707031\n",
      "Training loss:  6.803865909576416\n",
      "Training loss:  6.813629150390625\n",
      "Training loss:  6.818414688110352\n",
      "Training loss:  6.813779830932617\n",
      "Training loss:  6.816000938415527\n",
      "Training loss:  6.81209135055542\n",
      "Training loss:  6.809085369110107\n",
      "Training loss:  6.805699348449707\n",
      "Training loss:  6.810070991516113\n",
      "Training loss:  6.805263519287109\n",
      "Training loss:  6.821892738342285\n",
      "Training loss:  6.810146331787109\n",
      "Training loss:  6.81467342376709\n",
      "Training loss:  6.811074256896973\n",
      "Training loss:  6.812530040740967\n",
      "Training loss:  6.8144683837890625\n",
      "Training loss:  6.805929183959961\n",
      "Training loss:  6.82051420211792\n",
      "Training loss:  6.819854259490967\n",
      "Training loss:  6.815906524658203\n",
      "Training loss:  6.812887191772461\n",
      "Training loss:  6.813557147979736\n",
      "Training loss:  6.809741973876953\n",
      "Training loss:  6.810712814331055\n",
      "Training loss:  6.808690071105957\n",
      "Training loss:  6.817986488342285\n",
      "Training loss:  6.817500114440918\n",
      "Training loss:  6.812196731567383\n",
      "Training loss:  6.8127055168151855\n",
      "Validation Batch Error:  6.812288586412825\n",
      "Training loss:  6.816497802734375\n",
      "Training loss:  6.806197166442871\n",
      "Training loss:  6.813449382781982\n",
      "Training loss:  6.817668914794922\n",
      "Training loss:  6.811205863952637\n",
      "Training loss:  6.815610408782959\n",
      "Training loss:  6.807742118835449\n",
      "Training loss:  6.8108110427856445\n",
      "Training loss:  6.815452575683594\n",
      "Training loss:  6.81091833114624\n",
      "Training loss:  6.814959526062012\n",
      "Training loss:  6.807719707489014\n",
      "Training loss:  6.799541473388672\n",
      "Training loss:  6.815241813659668\n",
      "Training loss:  6.815493583679199\n",
      "Training loss:  6.817037582397461\n",
      "Training loss:  6.8071441650390625\n",
      "Training loss:  6.807156562805176\n",
      "Training loss:  6.816444396972656\n",
      "Training loss:  6.8181352615356445\n",
      "Training loss:  6.814148426055908\n",
      "Training loss:  6.822624206542969\n",
      "Training loss:  6.813587665557861\n",
      "Training loss:  6.810286045074463\n",
      "Training loss:  6.815336227416992\n",
      "Training loss:  6.806325912475586\n",
      "Training loss:  6.811763763427734\n",
      "Training loss:  6.813114166259766\n",
      "Training loss:  6.809085845947266\n",
      "Training loss:  6.811816692352295\n",
      "Training loss:  6.795463562011719\n",
      "Training loss:  6.8017802238464355\n",
      "Validation Batch Error:  6.812270978009444\n",
      "Training loss:  6.813641548156738\n",
      "Training loss:  6.8160600662231445\n",
      "Training loss:  6.819428443908691\n",
      "Training loss:  6.8169074058532715\n",
      "Training loss:  6.816222190856934\n",
      "Training loss:  6.816144943237305\n",
      "Training loss:  6.815036773681641\n",
      "Training loss:  6.81298828125\n",
      "Training loss:  6.814036846160889\n",
      "Training loss:  6.815964221954346\n",
      "Training loss:  6.807111740112305\n",
      "Training loss:  6.809175491333008\n",
      "Training loss:  6.815131187438965\n",
      "Training loss:  6.814527988433838\n",
      "Training loss:  6.802467346191406\n",
      "Training loss:  6.817408084869385\n",
      "Training loss:  6.815705299377441\n",
      "Training loss:  6.82039737701416\n",
      "Training loss:  6.8149542808532715\n",
      "Training loss:  6.8131561279296875\n",
      "Training loss:  6.8105669021606445\n",
      "Training loss:  6.811973571777344\n",
      "Training loss:  6.809429168701172\n",
      "Training loss:  6.815990447998047\n",
      "Training loss:  6.81455659866333\n",
      "Training loss:  6.81792688369751\n",
      "Training loss:  6.815401077270508\n",
      "Training loss:  6.811502933502197\n",
      "Training loss:  6.817056179046631\n",
      "Training loss:  6.80622673034668\n",
      "Training loss:  6.813983917236328\n",
      "Training loss:  6.807132720947266\n",
      "Validation Batch Error:  6.812246685165117\n",
      "Training loss:  6.807184219360352\n",
      "Training loss:  6.816514015197754\n",
      "Training loss:  6.804259300231934\n",
      "Training loss:  6.807734489440918\n",
      "Training loss:  6.81496524810791\n",
      "Training loss:  6.815593242645264\n",
      "Training loss:  6.81261682510376\n",
      "Training loss:  6.81306791305542\n",
      "Training loss:  6.805639266967773\n",
      "Training loss:  6.812501430511475\n",
      "Training loss:  6.814975738525391\n",
      "Training loss:  6.814460754394531\n",
      "Training loss:  6.810029983520508\n",
      "Training loss:  6.810973167419434\n",
      "Training loss:  6.816967010498047\n",
      "Training loss:  6.807710647583008\n",
      "Training loss:  6.811790466308594\n",
      "Training loss:  6.811624526977539\n",
      "Training loss:  6.80893611907959\n",
      "Training loss:  6.826127529144287\n",
      "Training loss:  6.8218512535095215\n",
      "Training loss:  6.805744647979736\n",
      "Training loss:  6.807916641235352\n",
      "Training loss:  6.8086700439453125\n",
      "Training loss:  6.81838846206665\n",
      "Training loss:  6.814428329467773\n",
      "Training loss:  6.8135480880737305\n",
      "Training loss:  6.811089992523193\n",
      "Training loss:  6.8051981925964355\n",
      "Training loss:  6.82009220123291\n",
      "Training loss:  6.812192440032959\n",
      "Training loss:  6.810112476348877\n",
      "Validation Batch Error:  6.812411741522073\n",
      "Training loss:  6.806882381439209\n",
      "Training loss:  6.815999507904053\n",
      "Training loss:  6.811979293823242\n",
      "Training loss:  6.815429210662842\n",
      "Training loss:  6.815982818603516\n",
      "Training loss:  6.816424369812012\n",
      "Training loss:  6.818897247314453\n",
      "Training loss:  6.811096668243408\n",
      "Training loss:  6.806276798248291\n",
      "Training loss:  6.811094760894775\n",
      "Training loss:  6.815118312835693\n",
      "Training loss:  6.8144426345825195\n",
      "Training loss:  6.804234981536865\n",
      "Training loss:  6.813508987426758\n",
      "Training loss:  6.822714328765869\n",
      "Training loss:  6.806802749633789\n",
      "Training loss:  6.818544387817383\n",
      "Training loss:  6.820906162261963\n",
      "Training loss:  6.8217949867248535\n",
      "Training loss:  6.813887119293213\n",
      "Training loss:  6.819498538970947\n",
      "Training loss:  6.8188300132751465\n",
      "Training loss:  6.8150482177734375\n",
      "Training loss:  6.806216239929199\n",
      "Training loss:  6.82086181640625\n",
      "Training loss:  6.81365442276001\n",
      "Training loss:  6.820399284362793\n",
      "Training loss:  6.808645248413086\n",
      "Training loss:  6.818827152252197\n",
      "Training loss:  6.821405410766602\n",
      "Training loss:  6.823782920837402\n",
      "Training loss:  6.8027424812316895\n",
      "Validation Batch Error:  6.812221048393723\n",
      "Training loss:  6.806676864624023\n",
      "Training loss:  6.810894966125488\n",
      "Training loss:  6.821403503417969\n",
      "Training loss:  6.821292400360107\n",
      "Training loss:  6.811609268188477\n",
      "Training loss:  6.808184623718262\n",
      "Training loss:  6.81352424621582\n",
      "Training loss:  6.808293342590332\n",
      "Training loss:  6.8091607093811035\n",
      "Training loss:  6.815024375915527\n",
      "Training loss:  6.8132405281066895\n",
      "Training loss:  6.814949989318848\n",
      "Training loss:  6.814939975738525\n",
      "Training loss:  6.810050964355469\n",
      "Training loss:  6.812039375305176\n",
      "Training loss:  6.813305854797363\n",
      "Training loss:  6.817391872406006\n",
      "Training loss:  6.816589832305908\n",
      "Training loss:  6.804169178009033\n",
      "Training loss:  6.810095310211182\n",
      "Training loss:  6.818068027496338\n",
      "Training loss:  6.812471866607666\n",
      "Training loss:  6.813502311706543\n",
      "Training loss:  6.807661056518555\n",
      "Training loss:  6.809138298034668\n",
      "Training loss:  6.815959453582764\n",
      "Training loss:  6.807275295257568\n",
      "Training loss:  6.8110575675964355\n",
      "Training loss:  6.808155059814453\n",
      "Training loss:  6.809628009796143\n",
      "Training loss:  6.815425872802734\n",
      "Training loss:  6.8117594718933105\n",
      "Validation Batch Error:  6.8123054021639735\n",
      "Training loss:  6.812214374542236\n",
      "Training loss:  6.820399761199951\n",
      "Training loss:  6.818864822387695\n",
      "Training loss:  6.811743259429932\n",
      "Training loss:  6.810605049133301\n",
      "Training loss:  6.815601825714111\n",
      "Training loss:  6.812963008880615\n",
      "Training loss:  6.809784889221191\n",
      "Training loss:  6.821390628814697\n",
      "Training loss:  6.81109619140625\n",
      "Training loss:  6.809065818786621\n",
      "Training loss:  6.812315940856934\n",
      "Training loss:  6.815929889678955\n",
      "Training loss:  6.813169479370117\n",
      "Training loss:  6.804758071899414\n",
      "Training loss:  6.814101219177246\n",
      "Training loss:  6.808631896972656\n",
      "Training loss:  6.821852207183838\n",
      "Training loss:  6.825308799743652\n",
      "Training loss:  6.809042453765869\n",
      "Training loss:  6.817197322845459\n",
      "Training loss:  6.8184285163879395\n",
      "Training loss:  6.815530300140381\n",
      "Training loss:  6.817905426025391\n",
      "Training loss:  6.8183770179748535\n",
      "Training loss:  6.818027973175049\n",
      "Training loss:  6.808309078216553\n",
      "Training loss:  6.803637504577637\n",
      "Training loss:  6.814107894897461\n",
      "Training loss:  6.813806533813477\n",
      "Training loss:  6.823197841644287\n",
      "Training loss:  6.809810638427734\n",
      "Validation Batch Error:  6.812249416311452\n",
      "Training loss:  6.818055152893066\n",
      "Training loss:  6.811990261077881\n",
      "Training loss:  6.819869041442871\n",
      "Training loss:  6.813642978668213\n",
      "Training loss:  6.8149495124816895\n",
      "Training loss:  6.815188407897949\n",
      "Training loss:  6.811512470245361\n",
      "Training loss:  6.811540603637695\n",
      "Training loss:  6.8135905265808105\n",
      "Training loss:  6.813035011291504\n",
      "Training loss:  6.8111572265625\n",
      "Training loss:  6.812134742736816\n",
      "Training loss:  6.818686008453369\n",
      "Training loss:  6.804375171661377\n",
      "Training loss:  6.811080455780029\n",
      "Training loss:  6.806710720062256\n",
      "Training loss:  6.814210414886475\n",
      "Training loss:  6.814050197601318\n",
      "Training loss:  6.8053812980651855\n",
      "Training loss:  6.80720329284668\n",
      "Training loss:  6.807186126708984\n",
      "Training loss:  6.814995765686035\n",
      "Training loss:  6.804751873016357\n",
      "Training loss:  6.812504768371582\n",
      "Training loss:  6.824717998504639\n",
      "Training loss:  6.812251567840576\n",
      "Training loss:  6.817388534545898\n",
      "Training loss:  6.807477951049805\n",
      "Training loss:  6.818422317504883\n",
      "Training loss:  6.814516067504883\n",
      "Training loss:  6.821378707885742\n",
      "Training loss:  6.8008832931518555\n",
      "Validation Batch Error:  6.812263022833288\n",
      "Training loss:  6.80930757522583\n",
      "Training loss:  6.818477153778076\n",
      "Training loss:  6.817884922027588\n",
      "Training loss:  6.811054229736328\n",
      "Training loss:  6.811044216156006\n",
      "Training loss:  6.816718101501465\n",
      "Training loss:  6.813624382019043\n",
      "Training loss:  6.80909538269043\n",
      "Training loss:  6.812053203582764\n",
      "Training loss:  6.808781623840332\n",
      "Training loss:  6.809694766998291\n",
      "Training loss:  6.820406436920166\n",
      "Training loss:  6.810675621032715\n",
      "Training loss:  6.803248405456543\n",
      "Training loss:  6.803913116455078\n",
      "Training loss:  6.810591697692871\n",
      "Training loss:  6.801320552825928\n",
      "Training loss:  6.8071136474609375\n",
      "Training loss:  6.807190895080566\n",
      "Training loss:  6.815471172332764\n",
      "Training loss:  6.808636665344238\n",
      "Training loss:  6.822902202606201\n",
      "Training loss:  6.816659927368164\n",
      "Training loss:  6.820337772369385\n",
      "Training loss:  6.810678958892822\n",
      "Training loss:  6.812204837799072\n",
      "Training loss:  6.802766799926758\n",
      "Training loss:  6.810660362243652\n",
      "Training loss:  6.821026802062988\n",
      "Training loss:  6.821747779846191\n",
      "Training loss:  6.812588691711426\n",
      "Training loss:  6.816681385040283\n",
      "Validation Batch Error:  6.8123618078574415\n",
      "Training loss:  6.809552192687988\n",
      "Training loss:  6.813551902770996\n",
      "Training loss:  6.809698581695557\n",
      "Training loss:  6.8091840744018555\n",
      "Training loss:  6.822812557220459\n",
      "Training loss:  6.804218769073486\n",
      "Training loss:  6.813372611999512\n",
      "Training loss:  6.81840705871582\n",
      "Training loss:  6.807615756988525\n",
      "Training loss:  6.807779312133789\n",
      "Training loss:  6.814082145690918\n",
      "Training loss:  6.803322792053223\n",
      "Training loss:  6.814410209655762\n",
      "Training loss:  6.812729835510254\n",
      "Training loss:  6.814026832580566\n",
      "Training loss:  6.817140579223633\n",
      "Training loss:  6.813011169433594\n",
      "Training loss:  6.815064430236816\n",
      "Training loss:  6.808152675628662\n",
      "Training loss:  6.809540748596191\n",
      "Training loss:  6.81225061416626\n",
      "Training loss:  6.7994537353515625\n",
      "Training loss:  6.813034534454346\n",
      "Training loss:  6.80814790725708\n",
      "Training loss:  6.80283260345459\n",
      "Training loss:  6.819901943206787\n",
      "Training loss:  6.821235656738281\n",
      "Training loss:  6.812632083892822\n",
      "Training loss:  6.808785438537598\n",
      "Training loss:  6.8185200691223145\n",
      "Training loss:  6.812588214874268\n",
      "Training loss:  6.803378105163574\n",
      "Validation Batch Error:  6.812263836041531\n",
      "Training loss:  6.8160080909729\n",
      "Training loss:  6.817026615142822\n",
      "Training loss:  6.813493728637695\n",
      "Training loss:  6.8161702156066895\n",
      "Training loss:  6.814538955688477\n",
      "Training loss:  6.8164963722229\n",
      "Training loss:  6.814554214477539\n",
      "Training loss:  6.819362640380859\n",
      "Training loss:  6.810685157775879\n",
      "Training loss:  6.810017108917236\n",
      "Training loss:  6.817420959472656\n",
      "Training loss:  6.810062885284424\n",
      "Training loss:  6.810736656188965\n",
      "Training loss:  6.818946838378906\n",
      "Training loss:  6.803754806518555\n",
      "Training loss:  6.803836345672607\n",
      "Training loss:  6.807651519775391\n",
      "Training loss:  6.8223795890808105\n",
      "Training loss:  6.814512252807617\n",
      "Training loss:  6.8096208572387695\n",
      "Training loss:  6.814949035644531\n",
      "Training loss:  6.810110092163086\n",
      "Training loss:  6.816938400268555\n",
      "Training loss:  6.8063154220581055\n",
      "Training loss:  6.803764343261719\n",
      "Training loss:  6.809682846069336\n",
      "Training loss:  6.808642387390137\n",
      "Training loss:  6.810032844543457\n",
      "Training loss:  6.809119701385498\n",
      "Training loss:  6.823732852935791\n",
      "Training loss:  6.812613487243652\n",
      "Training loss:  6.811559200286865\n",
      "Validation Batch Error:  6.812280568945727\n",
      "Training loss:  6.815072059631348\n",
      "Training loss:  6.812131404876709\n",
      "Training loss:  6.816037654876709\n",
      "Training loss:  6.815033912658691\n",
      "Training loss:  6.81260347366333\n",
      "Training loss:  6.823718070983887\n",
      "Training loss:  6.809145927429199\n",
      "Training loss:  6.820301055908203\n",
      "Training loss:  6.812533378601074\n",
      "Training loss:  6.815093994140625\n",
      "Training loss:  6.812648296356201\n",
      "Training loss:  6.8184309005737305\n",
      "Training loss:  6.817189693450928\n",
      "Training loss:  6.807650566101074\n",
      "Training loss:  6.813060760498047\n",
      "Training loss:  6.807720184326172\n",
      "Training loss:  6.811655044555664\n",
      "Training loss:  6.807381629943848\n",
      "Training loss:  6.8155951499938965\n",
      "Training loss:  6.81199836730957\n",
      "Training loss:  6.815920352935791\n",
      "Training loss:  6.812544345855713\n",
      "Training loss:  6.818365573883057\n",
      "Training loss:  6.8162078857421875\n",
      "Training loss:  6.82186222076416\n",
      "Training loss:  6.8101301193237305\n",
      "Training loss:  6.80875301361084\n",
      "Training loss:  6.813743591308594\n",
      "Training loss:  6.823850631713867\n",
      "Training loss:  6.803416728973389\n",
      "Training loss:  6.815452575683594\n",
      "Training loss:  6.811197757720947\n",
      "Validation Batch Error:  6.812434560874176\n",
      "Training loss:  6.815296173095703\n",
      "Training loss:  6.814977645874023\n",
      "Training loss:  6.821031093597412\n",
      "Training loss:  6.819474220275879\n",
      "Training loss:  6.813706874847412\n",
      "Training loss:  6.814696788787842\n",
      "Training loss:  6.8119916915893555\n",
      "Training loss:  6.806891918182373\n",
      "Training loss:  6.823707103729248\n",
      "Training loss:  6.809793472290039\n",
      "Training loss:  6.8188581466674805\n",
      "Training loss:  6.815518856048584\n",
      "Training loss:  6.813011169433594\n",
      "Training loss:  6.81044864654541\n",
      "Training loss:  6.805307865142822\n",
      "Training loss:  6.81117582321167\n",
      "Training loss:  6.8106865882873535\n",
      "Training loss:  6.806166648864746\n",
      "Training loss:  6.810956954956055\n",
      "Training loss:  6.81494665145874\n",
      "Training loss:  6.810643196105957\n",
      "Training loss:  6.815209865570068\n",
      "Training loss:  6.813458442687988\n",
      "Training loss:  6.803232669830322\n",
      "Training loss:  6.810181140899658\n",
      "Training loss:  6.813127517700195\n",
      "Training loss:  6.807294845581055\n",
      "Training loss:  6.810072898864746\n",
      "Training loss:  6.825738430023193\n",
      "Training loss:  6.816514015197754\n",
      "Training loss:  6.817482948303223\n",
      "Training loss:  6.810632228851318\n",
      "Validation Batch Error:  6.812265681722131\n",
      "Training loss:  6.805694103240967\n",
      "Training loss:  6.811537265777588\n",
      "Training loss:  6.80532693862915\n",
      "Training loss:  6.820945739746094\n",
      "Training loss:  6.808199405670166\n",
      "Training loss:  6.816627502441406\n",
      "Training loss:  6.8090949058532715\n",
      "Training loss:  6.8116044998168945\n",
      "Training loss:  6.813121318817139\n",
      "Training loss:  6.821889400482178\n",
      "Training loss:  6.80958366394043\n",
      "Training loss:  6.806291580200195\n",
      "Training loss:  6.813663005828857\n",
      "Training loss:  6.811024188995361\n",
      "Training loss:  6.8126702308654785\n",
      "Training loss:  6.817899227142334\n",
      "Training loss:  6.808314323425293\n",
      "Training loss:  6.814063549041748\n",
      "Training loss:  6.811683654785156\n",
      "Training loss:  6.816933631896973\n",
      "Training loss:  6.8101348876953125\n",
      "Training loss:  6.820868492126465\n",
      "Training loss:  6.810297966003418\n",
      "Training loss:  6.81165885925293\n",
      "Training loss:  6.8155035972595215\n",
      "Training loss:  6.814457893371582\n",
      "Training loss:  6.81008243560791\n",
      "Training loss:  6.82232666015625\n",
      "Training loss:  6.811670303344727\n",
      "Training loss:  6.8028998374938965\n",
      "Training loss:  6.814064025878906\n",
      "Training loss:  6.8185834884643555\n",
      "Validation Batch Error:  6.812238328212387\n",
      "Training loss:  6.816994667053223\n",
      "Training loss:  6.809143543243408\n",
      "Training loss:  6.8181047439575195\n",
      "Training loss:  6.819879055023193\n",
      "Training loss:  6.8134589195251465\n",
      "Training loss:  6.808608531951904\n",
      "Training loss:  6.815526485443115\n",
      "Training loss:  6.80232572555542\n",
      "Training loss:  6.807297229766846\n",
      "Training loss:  6.811118125915527\n",
      "Training loss:  6.816465854644775\n",
      "Training loss:  6.806435585021973\n",
      "Training loss:  6.810766220092773\n",
      "Training loss:  6.811334133148193\n",
      "Training loss:  6.813047885894775\n",
      "Training loss:  6.809232711791992\n",
      "Training loss:  6.816458702087402\n",
      "Training loss:  6.810496807098389\n",
      "Training loss:  6.811216354370117\n",
      "Training loss:  6.809180736541748\n",
      "Training loss:  6.80955696105957\n",
      "Training loss:  6.809243202209473\n",
      "Training loss:  6.815464019775391\n",
      "Training loss:  6.819854736328125\n",
      "Training loss:  6.8115997314453125\n",
      "Training loss:  6.8144941329956055\n",
      "Training loss:  6.8180084228515625\n",
      "Training loss:  6.814666271209717\n",
      "Training loss:  6.810234069824219\n",
      "Training loss:  6.813996315002441\n",
      "Training loss:  6.8117170333862305\n",
      "Training loss:  6.812292575836182\n",
      "Validation Batch Error:  6.812410888447627\n",
      "Training loss:  6.819112300872803\n",
      "Training loss:  6.815003871917725\n",
      "Training loss:  6.814007759094238\n",
      "Training loss:  6.8125176429748535\n",
      "Training loss:  6.811699390411377\n",
      "Training loss:  6.805754661560059\n",
      "Training loss:  6.817902088165283\n",
      "Training loss:  6.818330764770508\n",
      "Training loss:  6.807413101196289\n",
      "Training loss:  6.805556774139404\n",
      "Training loss:  6.8141021728515625\n",
      "Training loss:  6.80586051940918\n",
      "Training loss:  6.817109107971191\n",
      "Training loss:  6.81455135345459\n",
      "Training loss:  6.810120105743408\n",
      "Training loss:  6.809206962585449\n",
      "Training loss:  6.804715156555176\n",
      "Training loss:  6.813023567199707\n",
      "Training loss:  6.813041687011719\n",
      "Training loss:  6.808108329772949\n",
      "Training loss:  6.8053879737854\n",
      "Training loss:  6.812680244445801\n",
      "Training loss:  6.806180953979492\n",
      "Training loss:  6.812373161315918\n",
      "Training loss:  6.811173439025879\n",
      "Training loss:  6.816918849945068\n",
      "Training loss:  6.806848049163818\n",
      "Training loss:  6.8185625076293945\n",
      "Training loss:  6.814402103424072\n",
      "Training loss:  6.808642387390137\n",
      "Training loss:  6.821366310119629\n",
      "Training loss:  6.809165954589844\n",
      "Validation Batch Error:  6.812187409416513\n",
      "Training loss:  6.816375255584717\n",
      "Training loss:  6.815518379211426\n",
      "Training loss:  6.822262763977051\n",
      "Training loss:  6.82186222076416\n",
      "Training loss:  6.809230327606201\n",
      "Training loss:  6.829133987426758\n",
      "Training loss:  6.811637878417969\n",
      "Training loss:  6.811095237731934\n",
      "Training loss:  6.805315017700195\n",
      "Training loss:  6.816551685333252\n",
      "Training loss:  6.816552639007568\n",
      "Training loss:  6.803159713745117\n",
      "Training loss:  6.8047566413879395\n",
      "Training loss:  6.817094802856445\n",
      "Training loss:  6.8067193031311035\n",
      "Training loss:  6.810196399688721\n",
      "Training loss:  6.821558952331543\n",
      "Training loss:  6.815009117126465\n",
      "Training loss:  6.808296203613281\n",
      "Training loss:  6.8086347579956055\n",
      "Training loss:  6.804773807525635\n",
      "Training loss:  6.818432807922363\n",
      "Training loss:  6.806147575378418\n",
      "Training loss:  6.8045549392700195\n",
      "Training loss:  6.8110527992248535\n",
      "Training loss:  6.814650058746338\n",
      "Training loss:  6.8056793212890625\n",
      "Training loss:  6.821758270263672\n",
      "Training loss:  6.811017990112305\n",
      "Training loss:  6.814136028289795\n",
      "Training loss:  6.820466041564941\n",
      "Training loss:  6.805857181549072\n",
      "Validation Batch Error:  6.812201842539144\n",
      "Training loss:  6.81363582611084\n",
      "Training loss:  6.818845748901367\n",
      "Training loss:  6.812098503112793\n",
      "Training loss:  6.811601161956787\n",
      "Training loss:  6.8155741691589355\n",
      "Training loss:  6.803908824920654\n",
      "Training loss:  6.810715675354004\n",
      "Training loss:  6.808944225311279\n",
      "Training loss:  6.805309772491455\n",
      "Training loss:  6.814589977264404\n",
      "Training loss:  6.8056864738464355\n",
      "Training loss:  6.812488079071045\n",
      "Training loss:  6.8165202140808105\n",
      "Training loss:  6.803251266479492\n",
      "Training loss:  6.811988353729248\n",
      "Training loss:  6.8139190673828125\n",
      "Training loss:  6.818087577819824\n",
      "Training loss:  6.817899227142334\n",
      "Training loss:  6.809080123901367\n",
      "Training loss:  6.816504955291748\n",
      "Training loss:  6.810060501098633\n",
      "Training loss:  6.8184943199157715\n",
      "Training loss:  6.813035488128662\n",
      "Training loss:  6.81787633895874\n",
      "Training loss:  6.808794021606445\n",
      "Training loss:  6.8049821853637695\n",
      "Training loss:  6.808080673217773\n",
      "Training loss:  6.818403244018555\n",
      "Training loss:  6.806260108947754\n",
      "Training loss:  6.811209678649902\n",
      "Training loss:  6.808708667755127\n",
      "Training loss:  6.804379463195801\n",
      "Validation Batch Error:  6.8120874854024915\n",
      "Training loss:  6.809486389160156\n",
      "Training loss:  6.824189186096191\n",
      "Training loss:  6.815976619720459\n",
      "Training loss:  6.8110246658325195\n",
      "Training loss:  6.817936897277832\n",
      "Training loss:  6.806800842285156\n",
      "Training loss:  6.811703681945801\n",
      "Training loss:  6.8102617263793945\n",
      "Training loss:  6.812122344970703\n",
      "Training loss:  6.815921783447266\n",
      "Training loss:  6.795722007751465\n",
      "Training loss:  6.817132472991943\n",
      "Training loss:  6.8067827224731445\n",
      "Training loss:  6.8116044998168945\n",
      "Training loss:  6.812032699584961\n",
      "Training loss:  6.818889141082764\n",
      "Training loss:  6.805634021759033\n",
      "Training loss:  6.822843074798584\n",
      "Training loss:  6.808778285980225\n",
      "Training loss:  6.809719085693359\n",
      "Training loss:  6.810731887817383\n",
      "Training loss:  6.807665824890137\n",
      "Training loss:  6.812514781951904\n",
      "Training loss:  6.814183712005615\n",
      "Training loss:  6.8179450035095215\n",
      "Training loss:  6.809700012207031\n",
      "Training loss:  6.802978992462158\n",
      "Training loss:  6.803837299346924\n",
      "Training loss:  6.812520980834961\n",
      "Training loss:  6.8023681640625\n",
      "Training loss:  6.818904876708984\n",
      "Training loss:  6.8116936683654785\n",
      "Validation Batch Error:  6.812227453459796\n",
      "Training loss:  6.819766998291016\n",
      "Training loss:  6.812473773956299\n",
      "Training loss:  6.826169013977051\n",
      "Training loss:  6.814440727233887\n",
      "Training loss:  6.814207077026367\n",
      "Training loss:  6.811068534851074\n",
      "Training loss:  6.811180114746094\n",
      "Training loss:  6.8180036544799805\n",
      "Training loss:  6.813083171844482\n",
      "Training loss:  6.804201126098633\n",
      "Training loss:  6.816969871520996\n",
      "Training loss:  6.818425178527832\n",
      "Training loss:  6.814157009124756\n",
      "Training loss:  6.811059951782227\n",
      "Training loss:  6.816554069519043\n",
      "Training loss:  6.814564228057861\n",
      "Training loss:  6.818821430206299\n",
      "Training loss:  6.807252407073975\n",
      "Training loss:  6.8130879402160645\n",
      "Training loss:  6.814014434814453\n",
      "Training loss:  6.810660362243652\n",
      "Training loss:  6.81364631652832\n",
      "Training loss:  6.805175304412842\n",
      "Training loss:  6.820874214172363\n",
      "Training loss:  6.8199567794799805\n",
      "Training loss:  6.819827556610107\n",
      "Training loss:  6.818951606750488\n",
      "Training loss:  6.812523365020752\n",
      "Training loss:  6.818416595458984\n",
      "Training loss:  6.823720932006836\n",
      "Training loss:  6.80377197265625\n",
      "Training loss:  6.813056468963623\n",
      "Validation Batch Error:  6.812235076936688\n",
      "Training loss:  6.815942287445068\n",
      "Training loss:  6.815190315246582\n",
      "Training loss:  6.815591335296631\n",
      "Training loss:  6.8103532791137695\n",
      "Training loss:  6.805903911590576\n",
      "Training loss:  6.816387176513672\n",
      "Training loss:  6.816971778869629\n",
      "Training loss:  6.812382698059082\n",
      "Training loss:  6.805551528930664\n",
      "Training loss:  6.805761814117432\n",
      "Training loss:  6.811734676361084\n",
      "Training loss:  6.815873146057129\n",
      "Training loss:  6.819331169128418\n",
      "Training loss:  6.811979293823242\n",
      "Training loss:  6.807806015014648\n",
      "Training loss:  6.823192596435547\n",
      "Training loss:  6.815952777862549\n",
      "Training loss:  6.811645030975342\n",
      "Training loss:  6.810494899749756\n",
      "Training loss:  6.811801433563232\n",
      "Training loss:  6.811094760894775\n",
      "Training loss:  6.810833930969238\n",
      "Training loss:  6.812361717224121\n",
      "Training loss:  6.8038411140441895\n",
      "Training loss:  6.810084342956543\n",
      "Training loss:  6.808629512786865\n",
      "Training loss:  6.814021587371826\n",
      "Training loss:  6.815993785858154\n",
      "Training loss:  6.817513465881348\n",
      "Training loss:  6.8048014640808105\n",
      "Training loss:  6.809628486633301\n",
      "Training loss:  6.807723522186279\n",
      "Validation Batch Error:  6.812376980996459\n",
      "Training loss:  6.817073822021484\n",
      "Training loss:  6.819861888885498\n",
      "Training loss:  6.8209733963012695\n",
      "Training loss:  6.8178391456604\n",
      "Training loss:  6.812675952911377\n",
      "Training loss:  6.820933818817139\n",
      "Training loss:  6.8096466064453125\n",
      "Training loss:  6.8164777755737305\n",
      "Training loss:  6.817432403564453\n",
      "Training loss:  6.811965465545654\n",
      "Training loss:  6.8146071434021\n",
      "Training loss:  6.806197643280029\n",
      "Training loss:  6.821878910064697\n",
      "Training loss:  6.816821098327637\n",
      "Training loss:  6.8163628578186035\n",
      "Training loss:  6.814981460571289\n",
      "Training loss:  6.821834564208984\n",
      "Training loss:  6.807268142700195\n",
      "Training loss:  6.807620048522949\n",
      "Training loss:  6.804208278656006\n",
      "Training loss:  6.809111595153809\n",
      "Training loss:  6.820383548736572\n",
      "Training loss:  6.813109397888184\n",
      "Training loss:  6.806793212890625\n",
      "Training loss:  6.807114124298096\n",
      "Training loss:  6.818901062011719\n",
      "Training loss:  6.806717395782471\n",
      "Training loss:  6.817569255828857\n",
      "Training loss:  6.812492370605469\n",
      "Training loss:  6.8094282150268555\n",
      "Training loss:  6.8119964599609375\n",
      "Training loss:  6.819066524505615\n",
      "Validation Batch Error:  6.812287831446612\n",
      "Training loss:  6.81305456161499\n",
      "Training loss:  6.822417736053467\n",
      "Training loss:  6.818591594696045\n",
      "Training loss:  6.810825824737549\n",
      "Training loss:  6.821892261505127\n",
      "Training loss:  6.814464569091797\n",
      "Training loss:  6.8094024658203125\n",
      "Training loss:  6.810625076293945\n",
      "Training loss:  6.821907997131348\n",
      "Training loss:  6.805204391479492\n",
      "Training loss:  6.806690692901611\n",
      "Training loss:  6.808618545532227\n",
      "Training loss:  6.806670665740967\n",
      "Training loss:  6.808307647705078\n",
      "Training loss:  6.806735038757324\n",
      "Training loss:  6.816458225250244\n",
      "Training loss:  6.809128761291504\n",
      "Training loss:  6.81119441986084\n",
      "Training loss:  6.806426525115967\n",
      "Training loss:  6.811026573181152\n",
      "Training loss:  6.811557292938232\n",
      "Training loss:  6.817850589752197\n",
      "Training loss:  6.80814266204834\n",
      "Training loss:  6.813988208770752\n",
      "Training loss:  6.820324897766113\n",
      "Training loss:  6.8159589767456055\n",
      "Training loss:  6.805652618408203\n",
      "Training loss:  6.809521198272705\n",
      "Training loss:  6.811241149902344\n",
      "Training loss:  6.807204723358154\n",
      "Training loss:  6.8126220703125\n",
      "Training loss:  6.81495475769043\n",
      "Validation Batch Error:  6.812321700588852\n",
      "Training loss:  6.816300868988037\n",
      "Training loss:  6.81790828704834\n",
      "Training loss:  6.815451622009277\n",
      "Training loss:  6.811069011688232\n",
      "Training loss:  6.806717872619629\n",
      "Training loss:  6.815546989440918\n",
      "Training loss:  6.813453197479248\n",
      "Training loss:  6.812512397766113\n",
      "Training loss:  6.819613933563232\n",
      "Training loss:  6.81239652633667\n",
      "Training loss:  6.81306266784668\n",
      "Training loss:  6.815496444702148\n",
      "Training loss:  6.818437576293945\n",
      "Training loss:  6.824888706207275\n",
      "Training loss:  6.816413402557373\n",
      "Training loss:  6.819872856140137\n",
      "Training loss:  6.811570167541504\n",
      "Training loss:  6.81505012512207\n",
      "Training loss:  6.816945552825928\n",
      "Training loss:  6.810169219970703\n",
      "Training loss:  6.807659149169922\n",
      "Training loss:  6.811185359954834\n",
      "Training loss:  6.803287029266357\n",
      "Training loss:  6.806148052215576\n",
      "Training loss:  6.813562393188477\n",
      "Training loss:  6.81891393661499\n",
      "Training loss:  6.828134536743164\n",
      "Training loss:  6.8077592849731445\n",
      "Training loss:  6.811575889587402\n",
      "Training loss:  6.81151008605957\n",
      "Training loss:  6.815489768981934\n",
      "Training loss:  6.812509059906006\n",
      "Validation Batch Error:  6.812243134892898\n",
      "Training loss:  6.8144450187683105\n",
      "Training loss:  6.809160232543945\n",
      "Training loss:  6.80034875869751\n",
      "Training loss:  6.816924095153809\n",
      "Training loss:  6.810755729675293\n",
      "Training loss:  6.81119441986084\n",
      "Training loss:  6.820976257324219\n",
      "Training loss:  6.811527729034424\n",
      "Training loss:  6.809808254241943\n",
      "Training loss:  6.814229488372803\n",
      "Training loss:  6.812596321105957\n",
      "Training loss:  6.817958831787109\n",
      "Training loss:  6.807787895202637\n",
      "Training loss:  6.801441192626953\n",
      "Training loss:  6.816205024719238\n",
      "Training loss:  6.816102027893066\n",
      "Training loss:  6.80858039855957\n",
      "Training loss:  6.812626838684082\n",
      "Training loss:  6.8077497482299805\n",
      "Training loss:  6.813119411468506\n",
      "Training loss:  6.81151008605957\n",
      "Training loss:  6.811526775360107\n",
      "Training loss:  6.809628009796143\n",
      "Training loss:  6.815526008605957\n",
      "Training loss:  6.813460826873779\n",
      "Training loss:  6.808206558227539\n",
      "Training loss:  6.822290420532227\n",
      "Training loss:  6.8148651123046875\n",
      "Training loss:  6.811576843261719\n",
      "Training loss:  6.818927764892578\n",
      "Training loss:  6.818408012390137\n",
      "Training loss:  6.821834564208984\n",
      "Validation Batch Error:  6.812229571663266\n",
      "Training loss:  6.817134857177734\n",
      "Training loss:  6.802364349365234\n",
      "Training loss:  6.8164777755737305\n",
      "Training loss:  6.831216812133789\n",
      "Training loss:  6.80568790435791\n",
      "Training loss:  6.81841516494751\n",
      "Training loss:  6.819393157958984\n",
      "Training loss:  6.8097100257873535\n",
      "Training loss:  6.81602144241333\n",
      "Training loss:  6.821282863616943\n",
      "Training loss:  6.808598041534424\n",
      "Training loss:  6.804535865783691\n",
      "Training loss:  6.810682773590088\n",
      "Training loss:  6.810132026672363\n",
      "Training loss:  6.817549705505371\n",
      "Training loss:  6.813978672027588\n",
      "Training loss:  6.806868076324463\n",
      "Training loss:  6.806875705718994\n",
      "Training loss:  6.809619903564453\n",
      "Training loss:  6.811130523681641\n",
      "Training loss:  6.812631607055664\n",
      "Training loss:  6.809597492218018\n",
      "Training loss:  6.819955348968506\n",
      "Training loss:  6.811135292053223\n",
      "Training loss:  6.811563968658447\n",
      "Training loss:  6.8146562576293945\n",
      "Training loss:  6.812023639678955\n",
      "Training loss:  6.812987327575684\n",
      "Training loss:  6.81503438949585\n",
      "Training loss:  6.814576625823975\n",
      "Training loss:  6.814496040344238\n",
      "Training loss:  6.816921710968018\n",
      "Validation Batch Error:  6.812245287979295\n",
      "Training loss:  6.814002990722656\n",
      "Training loss:  6.809619903564453\n",
      "Training loss:  6.810183525085449\n",
      "Training loss:  6.81752872467041\n",
      "Training loss:  6.807621002197266\n",
      "Training loss:  6.814487457275391\n",
      "Training loss:  6.818363666534424\n",
      "Training loss:  6.799440860748291\n",
      "Training loss:  6.813077449798584\n",
      "Training loss:  6.817378997802734\n",
      "Training loss:  6.802215576171875\n",
      "Training loss:  6.805940628051758\n",
      "Training loss:  6.812519073486328\n",
      "Training loss:  6.799826145172119\n",
      "Training loss:  6.810129642486572\n",
      "Training loss:  6.813239097595215\n",
      "Training loss:  6.80908727645874\n",
      "Training loss:  6.8134894371032715\n",
      "Training loss:  6.810204982757568\n",
      "Training loss:  6.806645393371582\n",
      "Training loss:  6.819935321807861\n",
      "Training loss:  6.813120365142822\n",
      "Training loss:  6.816183567047119\n",
      "Training loss:  6.816979885101318\n",
      "Training loss:  6.811483383178711\n",
      "Training loss:  6.806188106536865\n",
      "Training loss:  6.812087535858154\n",
      "Training loss:  6.814048767089844\n",
      "Training loss:  6.821954727172852\n",
      "Training loss:  6.814720153808594\n",
      "Training loss:  6.817042827606201\n",
      "Training loss:  6.804910659790039\n",
      "Validation Batch Error:  6.81219214041353\n",
      "Training loss:  6.815871238708496\n",
      "Training loss:  6.807773590087891\n",
      "Training loss:  6.813490390777588\n",
      "Training loss:  6.813024044036865\n",
      "Training loss:  6.815087795257568\n",
      "Training loss:  6.8091349601745605\n",
      "Training loss:  6.81651496887207\n",
      "Training loss:  6.812053203582764\n",
      "Training loss:  6.80764102935791\n",
      "Training loss:  6.814986705780029\n",
      "Training loss:  6.804410934448242\n",
      "Training loss:  6.815889835357666\n",
      "Training loss:  6.816372871398926\n",
      "Training loss:  6.811656475067139\n",
      "Training loss:  6.817567348480225\n",
      "Training loss:  6.812107086181641\n",
      "Training loss:  6.8103766441345215\n",
      "Training loss:  6.814922332763672\n",
      "Training loss:  6.8155131340026855\n",
      "Training loss:  6.809194564819336\n",
      "Training loss:  6.820908069610596\n",
      "Training loss:  6.8178510665893555\n",
      "Training loss:  6.808640956878662\n",
      "Training loss:  6.812044620513916\n",
      "Training loss:  6.819963455200195\n",
      "Training loss:  6.820024490356445\n",
      "Training loss:  6.8048176765441895\n",
      "Training loss:  6.799858093261719\n",
      "Training loss:  6.8105244636535645\n",
      "Training loss:  6.819944858551025\n",
      "Training loss:  6.809726238250732\n",
      "Training loss:  6.816979885101318\n",
      "Validation Batch Error:  6.81227784963467\n",
      "Training loss:  6.809613227844238\n",
      "Training loss:  6.808216094970703\n",
      "Training loss:  6.818011283874512\n",
      "Training loss:  6.8077192306518555\n",
      "Training loss:  6.813042640686035\n",
      "Training loss:  6.808591842651367\n",
      "Training loss:  6.816504955291748\n",
      "Training loss:  6.817933559417725\n",
      "Training loss:  6.815001010894775\n",
      "Training loss:  6.80905294418335\n",
      "Training loss:  6.822310447692871\n",
      "Training loss:  6.809469699859619\n",
      "Training loss:  6.813573837280273\n",
      "Training loss:  6.814088344573975\n",
      "Training loss:  6.813584804534912\n",
      "Training loss:  6.813488006591797\n",
      "Training loss:  6.810541152954102\n",
      "Training loss:  6.816118240356445\n",
      "Training loss:  6.818515300750732\n",
      "Training loss:  6.809384822845459\n",
      "Training loss:  6.815222263336182\n",
      "Training loss:  6.81980037689209\n",
      "Training loss:  6.805351257324219\n",
      "Training loss:  6.817410469055176\n",
      "Training loss:  6.808213710784912\n",
      "Training loss:  6.809310436248779\n",
      "Training loss:  6.812101364135742\n",
      "Training loss:  6.811200141906738\n",
      "Training loss:  6.811126708984375\n",
      "Training loss:  6.816108703613281\n",
      "Training loss:  6.801353931427002\n",
      "Training loss:  6.808112144470215\n",
      "Validation Batch Error:  6.812272669519964\n",
      "Training loss:  6.813109397888184\n",
      "Training loss:  6.811790466308594\n",
      "Training loss:  6.805844306945801\n",
      "Training loss:  6.81709623336792\n",
      "Training loss:  6.816921234130859\n",
      "Training loss:  6.812686920166016\n",
      "Training loss:  6.805943012237549\n",
      "Training loss:  6.805839538574219\n",
      "Training loss:  6.815463542938232\n",
      "Training loss:  6.821279048919678\n",
      "Training loss:  6.813591957092285\n",
      "Training loss:  6.8130598068237305\n",
      "Training loss:  6.80968713760376\n",
      "Training loss:  6.813622951507568\n",
      "Training loss:  6.80678653717041\n",
      "Training loss:  6.815510272979736\n",
      "Training loss:  6.814725875854492\n",
      "Training loss:  6.80672025680542\n",
      "Training loss:  6.816449165344238\n",
      "Training loss:  6.810556411743164\n",
      "Training loss:  6.814515113830566\n",
      "Training loss:  6.817177772521973\n",
      "Training loss:  6.81411600112915\n",
      "Training loss:  6.813116073608398\n",
      "Training loss:  6.813196659088135\n",
      "Training loss:  6.816967010498047\n",
      "Training loss:  6.8076982498168945\n",
      "Training loss:  6.815688133239746\n",
      "Training loss:  6.813913822174072\n",
      "Training loss:  6.819713115692139\n",
      "Training loss:  6.818120956420898\n",
      "Training loss:  6.805368900299072\n",
      "Validation Batch Error:  6.8121972544998375\n",
      "Training loss:  6.810299873352051\n",
      "Training loss:  6.817937850952148\n",
      "Training loss:  6.822353363037109\n",
      "Training loss:  6.816474914550781\n",
      "Training loss:  6.815932273864746\n",
      "Training loss:  6.8118085861206055\n",
      "Training loss:  6.8091325759887695\n",
      "Training loss:  6.810208320617676\n",
      "Training loss:  6.8091349601745605\n",
      "Training loss:  6.819754600524902\n",
      "Training loss:  6.812116622924805\n",
      "Training loss:  6.814929008483887\n",
      "Training loss:  6.8139872550964355\n",
      "Training loss:  6.8137688636779785\n",
      "Training loss:  6.814435005187988\n",
      "Training loss:  6.822911262512207\n",
      "Training loss:  6.815090179443359\n",
      "Training loss:  6.806735038757324\n",
      "Training loss:  6.8066935539245605\n",
      "Training loss:  6.812604904174805\n",
      "Training loss:  6.8190531730651855\n",
      "Training loss:  6.805201530456543\n",
      "Training loss:  6.806719779968262\n",
      "Training loss:  6.815415382385254\n",
      "Training loss:  6.816816806793213\n",
      "Training loss:  6.815897464752197\n",
      "Training loss:  6.809627056121826\n",
      "Training loss:  6.809620380401611\n",
      "Training loss:  6.820967674255371\n",
      "Training loss:  6.821727752685547\n",
      "Training loss:  6.812613010406494\n",
      "Training loss:  6.8042216300964355\n",
      "Validation Batch Error:  6.812300860531419\n",
      "Training loss:  6.81451940536499\n",
      "Training loss:  6.807395935058594\n",
      "Training loss:  6.814096450805664\n",
      "Training loss:  6.818406105041504\n",
      "Training loss:  6.8115973472595215\n",
      "Training loss:  6.811275482177734\n",
      "Training loss:  6.8114728927612305\n",
      "Training loss:  6.812121391296387\n",
      "Training loss:  6.817070007324219\n",
      "Training loss:  6.818892002105713\n",
      "Training loss:  6.811185359954834\n",
      "Training loss:  6.810673713684082\n",
      "Training loss:  6.803375244140625\n",
      "Training loss:  6.806717872619629\n",
      "Training loss:  6.803295135498047\n",
      "Training loss:  6.825658321380615\n",
      "Training loss:  6.808582305908203\n",
      "Training loss:  6.810682773590088\n",
      "Training loss:  6.809756755828857\n",
      "Training loss:  6.815522193908691\n",
      "Training loss:  6.80911111831665\n",
      "Training loss:  6.804740905761719\n",
      "Training loss:  6.813972473144531\n",
      "Training loss:  6.81298828125\n",
      "Training loss:  6.807497024536133\n",
      "Training loss:  6.807946681976318\n",
      "Training loss:  6.810145378112793\n",
      "Training loss:  6.812052249908447\n",
      "Training loss:  6.80733585357666\n",
      "Training loss:  6.820013046264648\n",
      "Training loss:  6.8218994140625\n",
      "Training loss:  6.814716339111328\n",
      "Validation Batch Error:  6.8122318085309805\n",
      "Training loss:  6.819934844970703\n",
      "Training loss:  6.8135247230529785\n",
      "Training loss:  6.81150484085083\n",
      "Training loss:  6.809974193572998\n",
      "Training loss:  6.816966533660889\n",
      "Training loss:  6.814997673034668\n",
      "Training loss:  6.81654691696167\n",
      "Training loss:  6.8149919509887695\n",
      "Training loss:  6.804752826690674\n",
      "Training loss:  6.812718868255615\n",
      "Training loss:  6.817259311676025\n",
      "Training loss:  6.805453300476074\n",
      "Training loss:  6.810542106628418\n",
      "Training loss:  6.8144378662109375\n",
      "Training loss:  6.811835765838623\n",
      "Training loss:  6.814044952392578\n",
      "Training loss:  6.81453275680542\n",
      "Training loss:  6.818385601043701\n",
      "Training loss:  6.813090801239014\n",
      "Training loss:  6.80295991897583\n",
      "Training loss:  6.816449165344238\n",
      "Training loss:  6.818918704986572\n",
      "Training loss:  6.80487060546875\n",
      "Training loss:  6.812520503997803\n",
      "Training loss:  6.81248664855957\n",
      "Training loss:  6.813937664031982\n",
      "Training loss:  6.810072422027588\n",
      "Training loss:  6.808300971984863\n",
      "Training loss:  6.81496000289917\n",
      "Training loss:  6.805715560913086\n",
      "Training loss:  6.814435958862305\n",
      "Training loss:  6.823705673217773\n",
      "Validation Batch Error:  6.812248898673727\n",
      "Training loss:  6.82133150100708\n",
      "Training loss:  6.809152603149414\n",
      "Training loss:  6.808238983154297\n",
      "Training loss:  6.818118572235107\n",
      "Training loss:  6.813089370727539\n",
      "Training loss:  6.813063621520996\n",
      "Training loss:  6.808596611022949\n",
      "Training loss:  6.808629512786865\n",
      "Training loss:  6.824864864349365\n",
      "Training loss:  6.8194804191589355\n",
      "Training loss:  6.821300506591797\n",
      "Training loss:  6.80965518951416\n",
      "Training loss:  6.814511775970459\n",
      "Training loss:  6.818460464477539\n",
      "Training loss:  6.810189247131348\n",
      "Training loss:  6.807629108428955\n",
      "Training loss:  6.808104991912842\n",
      "Training loss:  6.812507152557373\n",
      "Training loss:  6.812552452087402\n",
      "Training loss:  6.808649063110352\n",
      "Training loss:  6.817265033721924\n",
      "Training loss:  6.8069047927856445\n",
      "Training loss:  6.824878215789795\n",
      "Training loss:  6.812202453613281\n",
      "Training loss:  6.815885066986084\n",
      "Training loss:  6.816991806030273\n",
      "Training loss:  6.816039562225342\n",
      "Training loss:  6.8288798332214355\n",
      "Training loss:  6.812713146209717\n",
      "Training loss:  6.818427562713623\n",
      "Training loss:  6.815947532653809\n",
      "Training loss:  6.813553333282471\n",
      "Validation Batch Error:  6.812318636185978\n",
      "Training loss:  6.805951118469238\n",
      "Training loss:  6.813164710998535\n",
      "Training loss:  6.808475971221924\n",
      "Training loss:  6.821859836578369\n",
      "Training loss:  6.809320449829102\n",
      "Training loss:  6.816957473754883\n",
      "Training loss:  6.813449859619141\n",
      "Training loss:  6.815213203430176\n",
      "Training loss:  6.804274559020996\n",
      "Training loss:  6.8052496910095215\n",
      "Training loss:  6.80535888671875\n",
      "Training loss:  6.802329063415527\n",
      "Training loss:  6.817380428314209\n",
      "Training loss:  6.813061714172363\n",
      "Training loss:  6.813248157501221\n",
      "Training loss:  6.812543869018555\n",
      "Training loss:  6.808188438415527\n",
      "Training loss:  6.802984714508057\n",
      "Training loss:  6.810606002807617\n",
      "Training loss:  6.809545516967773\n",
      "Training loss:  6.812073707580566\n",
      "Training loss:  6.808155536651611\n",
      "Training loss:  6.802780628204346\n",
      "Training loss:  6.814499855041504\n",
      "Training loss:  6.825710296630859\n",
      "Training loss:  6.810083866119385\n",
      "Training loss:  6.814163684844971\n",
      "Training loss:  6.817432880401611\n",
      "Training loss:  6.814022064208984\n",
      "Training loss:  6.815513610839844\n",
      "Training loss:  6.814050674438477\n",
      "Training loss:  6.808149337768555\n",
      "Validation Batch Error:  6.81230073096626\n",
      "Training loss:  6.810580253601074\n",
      "Training loss:  6.814557075500488\n",
      "Training loss:  6.814994812011719\n",
      "Training loss:  6.815903663635254\n",
      "Training loss:  6.811994552612305\n",
      "Training loss:  6.8150634765625\n",
      "Training loss:  6.807412147521973\n",
      "Training loss:  6.810277938842773\n",
      "Training loss:  6.814586639404297\n",
      "Training loss:  6.814992904663086\n",
      "Training loss:  6.814838409423828\n",
      "Training loss:  6.8105950355529785\n",
      "Training loss:  6.803825378417969\n",
      "Training loss:  6.824256420135498\n",
      "Training loss:  6.807292938232422\n",
      "Training loss:  6.803854942321777\n",
      "Training loss:  6.816027641296387\n",
      "Training loss:  6.806407928466797\n",
      "Training loss:  6.810216426849365\n",
      "Training loss:  6.821592330932617\n",
      "Training loss:  6.821803569793701\n",
      "Training loss:  6.81032657623291\n",
      "Training loss:  6.807097434997559\n",
      "Training loss:  6.8061347007751465\n",
      "Training loss:  6.818506240844727\n",
      "Training loss:  6.806177616119385\n",
      "Training loss:  6.814981460571289\n",
      "Training loss:  6.814574718475342\n",
      "Training loss:  6.8198113441467285\n",
      "Training loss:  6.808188438415527\n",
      "Training loss:  6.808143138885498\n",
      "Training loss:  6.810822010040283\n",
      "Validation Batch Error:  6.8124254922474385\n",
      "Training loss:  6.818587779998779\n",
      "Training loss:  6.814010143280029\n",
      "Training loss:  6.814547538757324\n",
      "Training loss:  6.817846775054932\n",
      "Training loss:  6.814092636108398\n",
      "Training loss:  6.807452201843262\n",
      "Training loss:  6.808879375457764\n",
      "Training loss:  6.823293209075928\n",
      "Training loss:  6.815949440002441\n",
      "Training loss:  6.817391395568848\n",
      "Training loss:  6.8101043701171875\n",
      "Training loss:  6.812196254730225\n",
      "Training loss:  6.821320056915283\n",
      "Training loss:  6.816080570220947\n",
      "Training loss:  6.819803714752197\n",
      "Training loss:  6.809080123901367\n",
      "Training loss:  6.805693626403809\n",
      "Training loss:  6.814095973968506\n",
      "Training loss:  6.810080051422119\n",
      "Training loss:  6.8159613609313965\n",
      "Training loss:  6.810153961181641\n",
      "Training loss:  6.807130336761475\n",
      "Training loss:  6.820501804351807\n",
      "Training loss:  6.810027599334717\n",
      "Training loss:  6.81454610824585\n",
      "Training loss:  6.815062046051025\n",
      "Training loss:  6.811178207397461\n",
      "Training loss:  6.813512802124023\n",
      "Training loss:  6.810627460479736\n",
      "Training loss:  6.817876815795898\n",
      "Training loss:  6.808225631713867\n",
      "Training loss:  6.81008243560791\n",
      "Validation Batch Error:  6.812237264906013\n",
      "Training loss:  6.813959121704102\n",
      "Training loss:  6.817978382110596\n",
      "Training loss:  6.818508625030518\n",
      "Training loss:  6.810588836669922\n",
      "Training loss:  6.8180389404296875\n",
      "Training loss:  6.80629301071167\n",
      "Training loss:  6.811624526977539\n",
      "Training loss:  6.817535400390625\n",
      "Training loss:  6.801910400390625\n",
      "Training loss:  6.816989898681641\n",
      "Training loss:  6.816964626312256\n",
      "Training loss:  6.821127414703369\n",
      "Training loss:  6.808071613311768\n",
      "Training loss:  6.813122749328613\n",
      "Training loss:  6.808352470397949\n",
      "Training loss:  6.811598777770996\n",
      "Training loss:  6.821994781494141\n",
      "Training loss:  6.817023277282715\n",
      "Training loss:  6.8159942626953125\n",
      "Training loss:  6.819835662841797\n",
      "Training loss:  6.813553810119629\n",
      "Training loss:  6.806298732757568\n",
      "Training loss:  6.817495822906494\n",
      "Training loss:  6.802411079406738\n",
      "Training loss:  6.808653831481934\n",
      "Training loss:  6.806789398193359\n",
      "Training loss:  6.813233375549316\n",
      "Training loss:  6.81451416015625\n",
      "Training loss:  6.804271221160889\n",
      "Training loss:  6.818370342254639\n",
      "Training loss:  6.821831703186035\n",
      "Training loss:  6.814113616943359\n",
      "Validation Batch Error:  6.812272732433815\n",
      "Training loss:  6.806193828582764\n",
      "Training loss:  6.817971706390381\n",
      "Training loss:  6.818437576293945\n",
      "Training loss:  6.813419818878174\n",
      "Training loss:  6.812704086303711\n",
      "Training loss:  6.815059185028076\n",
      "Training loss:  6.810884475708008\n",
      "Training loss:  6.807202339172363\n",
      "Training loss:  6.804392337799072\n",
      "Training loss:  6.805723190307617\n",
      "Training loss:  6.808095932006836\n",
      "Training loss:  6.805695533752441\n",
      "Training loss:  6.807829856872559\n",
      "Training loss:  6.817338466644287\n",
      "Training loss:  6.815615653991699\n",
      "Training loss:  6.813154697418213\n",
      "Training loss:  6.812119483947754\n",
      "Training loss:  6.810279369354248\n",
      "Training loss:  6.816993713378906\n",
      "Training loss:  6.813495635986328\n",
      "Training loss:  6.812623500823975\n",
      "Training loss:  6.80627965927124\n",
      "Training loss:  6.820797920227051\n",
      "Training loss:  6.807390213012695\n",
      "Training loss:  6.805242538452148\n",
      "Training loss:  6.809853553771973\n",
      "Training loss:  6.811082363128662\n",
      "Training loss:  6.810121059417725\n",
      "Training loss:  6.818428993225098\n",
      "Training loss:  6.818382740020752\n",
      "Training loss:  6.808176040649414\n",
      "Training loss:  6.812469482421875\n",
      "Validation Batch Error:  6.812280032932175\n",
      "Training loss:  6.8165082931518555\n",
      "Training loss:  6.813103199005127\n",
      "Training loss:  6.812557220458984\n",
      "Training loss:  6.805437088012695\n",
      "Training loss:  6.81026029586792\n",
      "Training loss:  6.814064979553223\n",
      "Training loss:  6.815067291259766\n",
      "Training loss:  6.8098249435424805\n",
      "Training loss:  6.807623386383057\n",
      "Training loss:  6.816412925720215\n",
      "Training loss:  6.812131404876709\n",
      "Training loss:  6.809632301330566\n",
      "Training loss:  6.8154802322387695\n",
      "Training loss:  6.813667297363281\n",
      "Training loss:  6.813560962677002\n",
      "Training loss:  6.813503742218018\n",
      "Training loss:  6.809358596801758\n",
      "Training loss:  6.812990665435791\n",
      "Training loss:  6.809042453765869\n",
      "Training loss:  6.809216499328613\n",
      "Training loss:  6.810661315917969\n",
      "Training loss:  6.81203556060791\n",
      "Training loss:  6.808670520782471\n",
      "Training loss:  6.810185432434082\n",
      "Training loss:  6.804862976074219\n",
      "Training loss:  6.807219505310059\n",
      "Training loss:  6.803816795349121\n",
      "Training loss:  6.80957555770874\n",
      "Training loss:  6.815030574798584\n",
      "Training loss:  6.799235820770264\n",
      "Training loss:  6.816392421722412\n",
      "Training loss:  6.810196399688721\n",
      "Validation Batch Error:  6.812306453635068\n",
      "Training loss:  6.810673236846924\n",
      "Training loss:  6.8126397132873535\n",
      "Training loss:  6.81611442565918\n",
      "Training loss:  6.813055992126465\n",
      "Training loss:  6.8165082931518555\n",
      "Training loss:  6.816916465759277\n",
      "Training loss:  6.809223175048828\n",
      "Training loss:  6.815426349639893\n",
      "Training loss:  6.811204433441162\n",
      "Training loss:  6.807172775268555\n",
      "Training loss:  6.814026355743408\n",
      "Training loss:  6.811739444732666\n",
      "Training loss:  6.811491966247559\n",
      "Training loss:  6.808736801147461\n",
      "Training loss:  6.811208724975586\n",
      "Training loss:  6.81689453125\n",
      "Training loss:  6.8190999031066895\n",
      "Training loss:  6.815509796142578\n",
      "Training loss:  6.80912971496582\n",
      "Training loss:  6.824972629547119\n",
      "Training loss:  6.809549331665039\n",
      "Training loss:  6.816043376922607\n",
      "Training loss:  6.801299571990967\n",
      "Training loss:  6.805739879608154\n",
      "Training loss:  6.814121246337891\n",
      "Training loss:  6.807369232177734\n",
      "Training loss:  6.815495491027832\n",
      "Training loss:  6.798885822296143\n",
      "Training loss:  6.81989860534668\n",
      "Training loss:  6.806890964508057\n",
      "Training loss:  6.8224263191223145\n",
      "Training loss:  6.806677341461182\n",
      "Validation Batch Error:  6.812183082064798\n",
      "Training loss:  6.814469337463379\n",
      "Training loss:  6.808286190032959\n",
      "Training loss:  6.824154376983643\n",
      "Training loss:  6.816539287567139\n",
      "Training loss:  6.808809280395508\n",
      "Training loss:  6.814966201782227\n",
      "Training loss:  6.810196399688721\n",
      "Training loss:  6.812547206878662\n",
      "Training loss:  6.809134006500244\n",
      "Training loss:  6.8121185302734375\n",
      "Training loss:  6.809338092803955\n",
      "Training loss:  6.8139801025390625\n",
      "Training loss:  6.819812774658203\n",
      "Training loss:  6.8086676597595215\n",
      "Training loss:  6.814528465270996\n",
      "Training loss:  6.813375949859619\n",
      "Training loss:  6.813042163848877\n",
      "Training loss:  6.812113285064697\n",
      "Training loss:  6.803812026977539\n",
      "Training loss:  6.80814266204834\n",
      "Training loss:  6.811770439147949\n",
      "Training loss:  6.807196617126465\n",
      "Training loss:  6.816838264465332\n",
      "Training loss:  6.814950942993164\n",
      "Training loss:  6.8080735206604\n",
      "Training loss:  6.81209135055542\n",
      "Training loss:  6.808194160461426\n",
      "Training loss:  6.809999465942383\n",
      "Training loss:  6.818012714385986\n",
      "Training loss:  6.818504333496094\n",
      "Training loss:  6.812302112579346\n",
      "Training loss:  6.820340156555176\n",
      "Validation Batch Error:  6.8123339186455985\n",
      "Training loss:  6.806674003601074\n",
      "Training loss:  6.809698104858398\n",
      "Training loss:  6.816400527954102\n",
      "Training loss:  6.810691833496094\n",
      "Training loss:  6.811629772186279\n",
      "Training loss:  6.8161211013793945\n",
      "Training loss:  6.813498497009277\n",
      "Training loss:  6.8052263259887695\n",
      "Training loss:  6.804708957672119\n",
      "Training loss:  6.80128288269043\n",
      "Training loss:  6.807798385620117\n",
      "Training loss:  6.806957244873047\n",
      "Training loss:  6.8110527992248535\n",
      "Training loss:  6.804320335388184\n",
      "Training loss:  6.810251235961914\n",
      "Training loss:  6.8037495613098145\n",
      "Training loss:  6.809989929199219\n",
      "Training loss:  6.810134410858154\n",
      "Training loss:  6.8159308433532715\n",
      "Training loss:  6.804848670959473\n",
      "Training loss:  6.804998397827148\n",
      "Training loss:  6.814493656158447\n",
      "Training loss:  6.819304466247559\n",
      "Training loss:  6.815250396728516\n",
      "Training loss:  6.8019843101501465\n",
      "Training loss:  6.807701110839844\n",
      "Training loss:  6.814753532409668\n",
      "Training loss:  6.8135271072387695\n",
      "Training loss:  6.807246685028076\n",
      "Training loss:  6.814157962799072\n",
      "Training loss:  6.811334133148193\n",
      "Training loss:  6.816529273986816\n",
      "Validation Batch Error:  6.8122986666444545\n",
      "Training loss:  6.807947158813477\n",
      "Training loss:  6.812991142272949\n",
      "Training loss:  6.815073013305664\n",
      "Training loss:  6.815481662750244\n",
      "Training loss:  6.811008453369141\n",
      "Training loss:  6.806985855102539\n",
      "Training loss:  6.802293300628662\n",
      "Training loss:  6.813024044036865\n",
      "Training loss:  6.815017223358154\n",
      "Training loss:  6.813332557678223\n",
      "Training loss:  6.816640377044678\n",
      "Training loss:  6.82028865814209\n",
      "Training loss:  6.8066511154174805\n",
      "Training loss:  6.815624713897705\n",
      "Training loss:  6.815916538238525\n",
      "Training loss:  6.808770656585693\n",
      "Training loss:  6.812997817993164\n",
      "Training loss:  6.809170246124268\n",
      "Training loss:  6.813773155212402\n",
      "Training loss:  6.812608242034912\n",
      "Training loss:  6.806190013885498\n",
      "Training loss:  6.805899143218994\n",
      "Training loss:  6.804168701171875\n",
      "Training loss:  6.811051368713379\n",
      "Training loss:  6.817955017089844\n",
      "Training loss:  6.809706211090088\n",
      "Training loss:  6.816780090332031\n",
      "Training loss:  6.814543724060059\n",
      "Training loss:  6.815740585327148\n",
      "Training loss:  6.814565181732178\n",
      "Training loss:  6.818906784057617\n",
      "Training loss:  6.818965435028076\n",
      "Validation Batch Error:  6.8123252654994415\n",
      "Training loss:  6.809638500213623\n",
      "Training loss:  6.820554256439209\n",
      "Training loss:  6.811484336853027\n",
      "Training loss:  6.813509941101074\n",
      "Training loss:  6.799010276794434\n",
      "Training loss:  6.8112711906433105\n",
      "Training loss:  6.820794105529785\n",
      "Training loss:  6.8130269050598145\n",
      "Training loss:  6.8179426193237305\n",
      "Training loss:  6.807219505310059\n",
      "Training loss:  6.807708740234375\n",
      "Training loss:  6.814513683319092\n",
      "Training loss:  6.807713985443115\n",
      "Training loss:  6.80664587020874\n",
      "Training loss:  6.816953659057617\n",
      "Training loss:  6.80423641204834\n",
      "Training loss:  6.8039350509643555\n",
      "Training loss:  6.815980911254883\n",
      "Training loss:  6.809689521789551\n",
      "Training loss:  6.805680751800537\n",
      "Training loss:  6.808403491973877\n",
      "Training loss:  6.81105899810791\n",
      "Training loss:  6.823423385620117\n",
      "Training loss:  6.813101768493652\n",
      "Training loss:  6.811615467071533\n",
      "Training loss:  6.805156230926514\n",
      "Training loss:  6.813760757446289\n",
      "Training loss:  6.806300640106201\n",
      "Training loss:  6.798573970794678\n",
      "Training loss:  6.803281784057617\n",
      "Training loss:  6.821961402893066\n",
      "Training loss:  6.812541961669922\n",
      "Validation Batch Error:  6.812244462001409\n",
      "Training loss:  6.811037540435791\n",
      "Training loss:  6.817070484161377\n",
      "Training loss:  6.815984725952148\n",
      "Training loss:  6.809664726257324\n",
      "Training loss:  6.80192756652832\n",
      "Training loss:  6.816004753112793\n",
      "Training loss:  6.814594268798828\n",
      "Training loss:  6.805987358093262\n",
      "Training loss:  6.81203556060791\n",
      "Training loss:  6.80776834487915\n",
      "Training loss:  6.810533046722412\n",
      "Training loss:  6.810760021209717\n",
      "Training loss:  6.816385269165039\n",
      "Training loss:  6.806911468505859\n",
      "Training loss:  6.813295364379883\n",
      "Training loss:  6.809581279754639\n",
      "Training loss:  6.8028154373168945\n",
      "Training loss:  6.814943790435791\n",
      "Training loss:  6.8111982345581055\n",
      "Training loss:  6.813526630401611\n",
      "Training loss:  6.813708782196045\n",
      "Training loss:  6.815482139587402\n",
      "Training loss:  6.821827411651611\n",
      "Training loss:  6.814563274383545\n",
      "Training loss:  6.814006805419922\n",
      "Training loss:  6.80914306640625\n",
      "Training loss:  6.808749675750732\n",
      "Training loss:  6.813234806060791\n",
      "Training loss:  6.815773010253906\n",
      "Training loss:  6.809680461883545\n",
      "Training loss:  6.815981864929199\n",
      "Training loss:  6.807264804840088\n",
      "Validation Batch Error:  6.81223155313812\n",
      "Training loss:  6.810606956481934\n",
      "Training loss:  6.813447952270508\n",
      "Training loss:  6.815972805023193\n",
      "Training loss:  6.806153774261475\n",
      "Training loss:  6.808228015899658\n",
      "Training loss:  6.804202079772949\n",
      "Training loss:  6.815110206604004\n",
      "Training loss:  6.810427665710449\n",
      "Training loss:  6.8027873039245605\n",
      "Training loss:  6.816867828369141\n",
      "Training loss:  6.817108154296875\n",
      "Training loss:  6.8125762939453125\n",
      "Training loss:  6.815608024597168\n",
      "Training loss:  6.806760787963867\n",
      "Training loss:  6.8165507316589355\n",
      "Training loss:  6.820075988769531\n",
      "Training loss:  6.804865837097168\n",
      "Training loss:  6.812091827392578\n",
      "Training loss:  6.810297966003418\n",
      "Training loss:  6.814097881317139\n",
      "Training loss:  6.80815315246582\n",
      "Training loss:  6.812543869018555\n",
      "Training loss:  6.81205940246582\n",
      "Training loss:  6.8155999183654785\n",
      "Training loss:  6.819375514984131\n",
      "Training loss:  6.8235859870910645\n",
      "Training loss:  6.800034523010254\n",
      "Training loss:  6.802802562713623\n",
      "Training loss:  6.804681301116943\n",
      "Training loss:  6.824370384216309\n",
      "Training loss:  6.808662414550781\n",
      "Training loss:  6.808151721954346\n",
      "Validation Batch Error:  6.812191257750887\n",
      "Training loss:  6.814929485321045\n",
      "Training loss:  6.80987024307251\n",
      "Training loss:  6.8125\n",
      "Training loss:  6.8117146492004395\n",
      "Training loss:  6.811672210693359\n",
      "Training loss:  6.813996315002441\n",
      "Training loss:  6.805451393127441\n",
      "Training loss:  6.806657314300537\n",
      "Training loss:  6.809573173522949\n",
      "Training loss:  6.809605121612549\n",
      "Training loss:  6.816486358642578\n",
      "Training loss:  6.813063621520996\n",
      "Training loss:  6.817603588104248\n",
      "Training loss:  6.812537670135498\n",
      "Training loss:  6.807352542877197\n",
      "Training loss:  6.812103748321533\n",
      "Training loss:  6.803712368011475\n",
      "Training loss:  6.82039737701416\n",
      "Training loss:  6.811695098876953\n",
      "Training loss:  6.819868087768555\n",
      "Training loss:  6.808586597442627\n",
      "Training loss:  6.820263385772705\n",
      "Training loss:  6.80777645111084\n",
      "Training loss:  6.813784122467041\n",
      "Training loss:  6.824721813201904\n",
      "Training loss:  6.821385860443115\n",
      "Training loss:  6.8063178062438965\n",
      "Training loss:  6.818484306335449\n",
      "Training loss:  6.8125505447387695\n",
      "Training loss:  6.816921234130859\n",
      "Training loss:  6.808952331542969\n",
      "Training loss:  6.805315017700195\n",
      "Validation Batch Error:  6.812397482502624\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "lst_train_loss, lst_valid_loss, model =  train(epochs, model, loss_fn, optim, train_dataloader, \n",
    "                                               val_dataloader, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac763060-675a-4952-b8db-68e424276581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T02:01:02.639920Z",
     "iopub.status.busy": "2022-07-06T02:01:02.639715Z",
     "iopub.status.idle": "2022-07-06T02:01:02.995604Z",
     "shell.execute_reply": "2022-07-06T02:01:02.994986Z",
     "shell.execute_reply.started": "2022-07-06T02:01:02.639898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2)\n",
    "train_batches = [i for i in range(100, 100*(len(lst_train_loss)+1),100)]\n",
    "test_epochs = [i for i in range(1,len(lst_valid_loss)+1)]\n",
    "ax[0].plot(train_batches[4:], lst_train_loss[4:])\n",
    "ax[0].scatter(train_batches[4:], lst_train_loss[4:])\n",
    "ax[0].set_title('Train Error By Training Batches')\n",
    "ax[0].set_xlabel('Minibatch Number')\n",
    "ax[0].set_ylabel('MSE Loss')\n",
    "\n",
    "ax[1].plot(test_epochs[4:],lst_valid_loss[4:])\n",
    "ax[1].scatter(test_epochs[4:],lst_valid_loss[4:])\n",
    "ax[1].set_title('Validation Error By Epochs')\n",
    "ax[1].set_xlabel('Epoch Number')\n",
    "ax[1].set_ylabel('MSE Loss')\n",
    "\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd1e11-45e1-4b99-bcd1-459ac3a374f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d797aba-2416-4d29-9aa9-7e79fd42f4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66b211dc-ea53-40f1-9285-9cd49dae7581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-06T02:01:02.996942Z",
     "iopub.status.busy": "2022-07-06T02:01:02.996756Z",
     "iopub.status.idle": "2022-07-06T02:01:03.590575Z",
     "shell.execute_reply": "2022-07-06T02:01:03.589455Z",
     "shell.execute_reply.started": "2022-07-06T02:01:02.996919Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb10fd-dc00-400f-b8de-419740ddf2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
